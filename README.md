# Building Regression models from sratch

This work is one of a series of two distinct repositories that collectively constitute the coursework focused on Classical Machine Learning methods for the Data Science & Information Technologies masters course: Îœ124 - Machine Learning at the National and Kapodistrian University of Athens (NKUA) during the Fall 2022 semester. The other repository deals with [Building Classifiers from scratch](https://github.com/KyriakosPsa/Classifiers-from-scratch).

# Overview

This repository contains an optimization generalized linear regression(GLM) problem solved from scratch using only the Numpy library. Throughout the development of this work, matrix operations with Numpy have been utilized whenever possible to ensure optimal performance. The code also includes certain classes and global functions that have been created from scratch to facilitate the implementation of the methodologies. Explanations of how these functions and classes work are explicitly mentioned and are provided when they are invoked in the code. Furthermore, additional comments on methodology and mathematical proofs are included in certain places, where they deemed necessary, to provide a deeper understanding of the solutions.

The solutions involve implementing with Numpy and analyzing the prediction bias-variance of the following methods:
---
- The Least Squares method
- The Ridge Regression method
- Full Bayesian Inference
- The Expectation-Maximization method

Additionally, the notebook will investigate the effects of utilizing:
---
- Different noise samples
- Different polynomial model complexities
- Prior parameter knowledge
---
The goal of this project is to compare and contrast the different models & methods mentioned and discuss the importance of balance between bias and variance concerning model predictions.
